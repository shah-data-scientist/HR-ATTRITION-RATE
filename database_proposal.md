## Database Schema Proposal

To ensure complete traceability of model inputs and outputs, and to store the employee data, I propose the following database schema:

### 1. `employees` Table
This table will store the raw, merged employee data from the `extrait_eval.csv`, `extrait_sirh.csv`, and `extrait_sondage.csv` files. This serves as the source of truth for employee information.

| Column Name                     | Data Type      | Constraints      | Description                                     |
| :------------------------------ | :------------- | :--------------- | :---------------------------------------------- |
| `id_employee`                   | INTEGER        | PRIMARY KEY      | Unique identifier for each employee             |
| `augmentation_salaire_precedente` | REAL           | NULLABLE         | Previous salary increase percentage             |
| `heures_supplementaires`        | INTEGER        | NULLABLE         | Overtime hours (0 for No, 1 for Yes)            |
| `genre`                         | INTEGER        | NULLABLE         | Gender (0 for Female, 1 for Male)               |
| `nombre_heures_travailless`     | REAL           | NULLABLE         | Number of hours worked                          |
| `note_evaluation_actuelle`      | REAL           | NULLABLE         | Current evaluation score                        |
| `note_evaluation_precedente`    | REAL           | NULLABLE         | Previous evaluation score                       |
| `satisfaction_employee_nature_travail` | REAL      | NULLABLE         | Employee satisfaction with work nature          |
| `satisfaction_employee_equipe`  | REAL           | NULLABLE         | Employee satisfaction with team                 |
| `satisfaction_employee_equilibre_pro_perso` | REAL | NULLABLE         | Employee satisfaction with work-life balance    |
| `annees_dans_le_poste_actuel`   | REAL           | NULLABLE         | Years in current role                           |
| `annees_dans_l_entreprise`      | REAL           | NULLABLE         | Years in the company                            |
| `date_ingestion`                | TIMESTAMP      | NOT NULL         | Timestamp when the record was ingested          |
| ... (other relevant columns from merged data) ... | ...            | ...              | ...                                             |

### 2. `model_inputs` Table
This table will store the preprocessed and engineered features that are fed into the machine learning model for prediction. Each row represents a single prediction request's input features.

| Column Name                     | Data Type      | Constraints      | Description                                     |
| :------------------------------ | :------------- | :--------------- | :---------------------------------------------- |
| `input_id`                      | SERIAL         | PRIMARY KEY      | Unique identifier for the model input           |
| `id_employee`                   | INTEGER        | NOT NULL, FK     | Foreign Key to `employees.id_employee`          |
| `feature_name_1`                | REAL / INTEGER | NOT NULL         | Preprocessed feature 1 (e.g., `age_scaled`)     |
| `feature_name_2`                | REAL / INTEGER | NOT NULL         | Preprocessed feature 2 (e.g., `gender_encoded`)|
| ... (all features used by the model) ... | ...            | NOT NULL         | ...                                             |
| `prediction_timestamp`          | TIMESTAMP      | NOT NULL         | Timestamp when this input was sent to the model |

### 3. `model_outputs` Table
This table will store the predictions generated by the machine learning model.

| Column Name                     | Data Type      | Constraints      | Description                                     |
| :------------------------------ | :------------- | :--------------- | :---------------------------------------------- |
| `output_id`                     | SERIAL         | PRIMARY KEY      | Unique identifier for the model output          |
| `prediction_proba`              | REAL           | NOT NULL         | Predicted probability of attrition              |
| `risk_category`                 | VARCHAR(50)    | NOT NULL         | Categorized risk (Low, Medium, High)            |
| `prediction_label`              | VARCHAR(50)    | NOT NULL         | Binary prediction (Stay, Leave)                 |
| `log_odds`                      | REAL           | NOT NULL         | Log-odds score (f(x))                           |
| `prediction_timestamp`          | TIMESTAMP      | NOT NULL         | Timestamp when this output was generated        |

### 4. `predictions_traceability` Table
This table links model inputs to their corresponding outputs and stores metadata for complete traceability.

| Column Name                     | Data Type      | Constraints      | Description                                     |
| :------------------------------ | :------------- | :--------------- | :---------------------------------------------- |
| `trace_id`                      | SERIAL         | PRIMARY KEY      | Unique identifier for the prediction trace      |
| `input_id`                      | INTEGER        | NOT NULL, FK     | Foreign Key to `model_inputs.input_id`          |
| `output_id`                     | INTEGER        | NOT NULL, FK     | Foreign Key to `model_outputs.output_id`        |
| `model_version`                 | VARCHAR(50)    | NULLABLE         | Version of the ML model used for prediction     |
| `prediction_source`             | VARCHAR(50)    | NULLABLE         | Source of the prediction (e.g., 'Streamlit App', 'FastAPI') |
| `request_metadata`              | JSONB          | NULLABLE         | Additional metadata about the prediction request (e.g., user ID, request ID) |
| `created_at`                    | TIMESTAMP      | NOT NULL         | Timestamp when this traceability record was created |

---

## Docker Compose Configuration (`docker-compose.yml`)

This file will define the PostgreSQL service.

```yaml
version: '3.8'

services:
  db:
    image: postgres:16-alpine
    restart: always
    environment:
      POSTGRES_DB: hr_attrition_db
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
    ports:
      - "5432:5432"
    volumes:
      - db_data:/var/lib/postgresql/data
      # You can add a volume for initialization scripts if needed
      # - ./init_db:/docker-entrypoint-initdb.d

volumes:
  db_data:
```

**Explanation:**
*   `version: '3.8'`: Specifies the Docker Compose file format version.
*   `services`: Defines the services that make up your application.
    *   `db`: This is the name of our PostgreSQL service.
        *   `image: postgres:16-alpine`: Uses the official PostgreSQL 16 image (alpine variant for smaller size).
        *   `restart: always`: Ensures the database container restarts if it stops.
        *   `environment`: Sets environment variables for the PostgreSQL container.
            *   `POSTGRES_DB`: The name of the database to create.
            *   `POSTGRES_USER`: The superuser for the database.
            *   `POSTGRES_PASSWORD`: The password for the superuser.
        *   `ports: - "5432:5432"`: Maps port 5432 on your host machine to port 5432 in the container, allowing you to connect to the database from your host.
        *   `volumes: - db_data:/var/lib/postgresql/data`: Persists the database data to a named volume `db_data` on your host, so data is not lost if the container is removed.
*   `volumes`: Defines the named volumes used by the services.

---

**Next Steps:**

1.  **User Approval:** Confirm if this schema and Docker Compose configuration meet your requirements.
2.  **Implement SQLAlchemy Models:** Create Python classes that map to these database tables using SQLAlchemy.
3.  **Database Initialization Script:** Write a script to create these tables in the PostgreSQL database.
4.  **Data Ingestion Script:** Write a script to load the `extrait_eval.csv`, `extrait_sirh.csv`, and `extrait_sondage.csv` into the `employees` table.
5.  **Integrate with `app.py` and `api/app/main.py`:** Modify the application logic to interact with the database for model inputs and outputs.
6.  **Testing and Documentation.**